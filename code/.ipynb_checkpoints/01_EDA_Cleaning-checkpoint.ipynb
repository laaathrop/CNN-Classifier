{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAo-Vfg82eHV"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# For functions pertaining to the operating system\n",
    "import os\n",
    "\n",
    "# Module for working with URLs\n",
    "import urllib\n",
    "\n",
    "# For performing operations on zip files\n",
    "import zipfile\n",
    "\n",
    "# Makes it possible to read and write tar archives including gzip, bz2 and lzma compression\n",
    "import tarfile\n",
    "\n",
    "# Module for working with URLs\n",
    "from six.moves import urllib\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.training import HParams\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Activation, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, AveragePooling2D, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Visualization\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSHs1DM74WO5"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "params = HParams(\n",
    "    \n",
    "    n_class=10, # Number of classes in the Cifar 10 dataset\n",
    "    learning_rate=1e-4, # How much to change the model in response to the estimated error each time the weights are updated\n",
    "    train_batch_size=32, # Training batch size; number of data points in one forward/backward pass\n",
    "    val_batch_size=32, # Batch size for validation; number of data points in one forward/backward pass\n",
    "    test_batch_size=32, # Testing batch size; number of data points in one forward/backward pass\n",
    "    n_epochs=10, # One epoch is equivalent to one forward and one backward pass of all training data points\n",
    "    input_name='input_one', # Name of NN input layer\n",
    "    data_dir='/tmp/data/', # Path to data\n",
    "    checkpoint_dir='/tmp/checkpoints', # Path to a location to save project checkpoints\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbvPG9w-3ZsH"
   },
   "outputs": [],
   "source": [
    "# Cifar 100 Dataset URL\n",
    "data_url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "\n",
    "# Dimension of the square image\n",
    "n_pixels = 32\n",
    "\n",
    "# Number of image color channels\n",
    "n_channels = 3\n",
    "\n",
    "# Length of flattened image\n",
    "size_flat = n_channels * (n_pixels**2)\n",
    "\n",
    "# Number of classes considered\n",
    "n_classes = params.n_class\n",
    "\n",
    "# Quantity of files in the original dataset\n",
    "_n_files_train = 5\n",
    "\n",
    "# Number of images per batch of training set\n",
    "_n_images_per_file = 10000\n",
    "\n",
    "# Quantity of training images\n",
    "_n_images_train = _n_files_train * _n_images_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soS11oLB3xRW"
   },
   "outputs": [],
   "source": [
    "def download(source_url, filename, destination):\n",
    "\n",
    "    \"\"\"\n",
    "    Description - Downloads the remote dataset to a local directory.\n",
    "\n",
    "    Returns - N/A\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the final file path\n",
    "    destination_path = os.path.join(destination, filename)\n",
    "\n",
    "    # If the file does not exist, \n",
    "    if not os.path.exists(destination_path):\n",
    "\n",
    "      # If the directory does not exist, \n",
    "      if not os.path.exists(destination):\n",
    "\n",
    "        # Create the needed directory \n",
    "        os.makedirs(destination_path)\n",
    "\n",
    "        # Print a status message\n",
    "        print(filename, \"currently downloading...\")\n",
    "\n",
    "        # Create URL by combining the base URL with the filename\n",
    "        url = source_url + filename\n",
    "\n",
    "        # Download the dataset using urllib\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url, filename=destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7yGviNy3z-6"
   },
   "outputs": [],
   "source": [
    "def download_and_extract(url=data_url, destination=params.data_dir):\n",
    "    \"\"\"\n",
    "    Description - Downloads and extracts the data from the data URL into\n",
    "                  the directory specified by the params class.\n",
    "\n",
    "    Returns - N/A\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the url using / as the delimiter, reverse order\n",
    "    filename = url.split('/')[-1]\n",
    "\n",
    "    # Create the final file path\n",
    "    destination_path = os.path.join(destination, filename)\n",
    "\n",
    "    # If the file does not exist, \n",
    "    if not os.path.exists(destination_path):\n",
    "\n",
    "        # If the directory does not exist,\n",
    "        if not os.path.exists(destination):\n",
    "\n",
    "            # Create the needed directory \n",
    "            os.makedirs(destination_path)\n",
    "\n",
    "        # Download the dataset using urllib\n",
    "        destination_path, _ = urllib.request.urlretrieve(url=url, filename=destination_path)\n",
    "\n",
    "        # Print a status message\n",
    "        print()\n",
    "        print(\"Step (1/2) - Files have been downloaded.\")\n",
    "\n",
    "        # If we are dealing with a zip file,\n",
    "        if destination_path.endswith(\".zip\"):\n",
    "\n",
    "            # extract the zipfile into the desitnation directory\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(destination)\n",
    "\n",
    "        # If we are dealing with a tar file, extract with tarfile\n",
    "        elif destination_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "\n",
    "            # Extract the tarfile into the desitnation directory\n",
    "            tarfile.open(name=destination_path, mode=\"r:gz\").extractall(destination)\n",
    "\n",
    "            # Print a status message\n",
    "            print(\"Step (2/2) - Files have been extracted.\")\n",
    "\n",
    "    # If the data does exist, \n",
    "    else:\n",
    "\n",
    "        # Print a status message\n",
    "        print(\"Data has apparently already been saved locally and extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iS13z6wB33tS"
   },
   "outputs": [],
   "source": [
    "def _get_file_path(filename=\"\"):\n",
    "    \"\"\"\n",
    "    Description - Finds and returns the data path location.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the join method to create the path\n",
    "    return os.path.join(params.data_dir, \"cifar-100-python/\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlK8OyrZ36Ct"
   },
   "outputs": [],
   "source": [
    "def _unpickle(filename):\n",
    "    \"\"\"\n",
    "    Description - Unpickle (de-serialize) the given file pieces and return the \n",
    "                  aggregated data chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the file path using the function defined above\n",
    "    file_path = _get_file_path(filename)\n",
    "\n",
    "    # Print a status message\n",
    "    print(\"Currently loading data from: \" + file_path)\n",
    "\n",
    "    # Open the file located at file_path\n",
    "    with open(file_path, mode='rb') as file:\n",
    "\n",
    "        # Load the data into a new variable using Pickle's load method\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbzdMpMd38X2"
   },
   "outputs": [],
   "source": [
    "def _convert_images(raw_data):\n",
    "    \"\"\"\n",
    "    Description - Preprocesses raw image data and convert to a 4-dimensional \n",
    "    array: [image_number, height, width, channel]\n",
    "   \n",
    "    Returns - The preprocessed and scaled image data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Scale the pixel data\n",
    "    scaled_data = np.array(raw_data, dtype=float) / 255.0\n",
    "\n",
    "    # Change the shape of the array to 4-D\n",
    "    images = scaled_data.reshape([-1, n_channels, img_size, img_size])\n",
    "\n",
    "    # Reindex the array\n",
    "    images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vk-XVMFt3-51"
   },
   "outputs": [],
   "source": [
    "def _load_data(filename):\n",
    "    \"\"\"\n",
    "    Description - Unpickles (de-serializes) the input file and converts the data\n",
    "                  to the data shape specified in the _convert_data function.\n",
    "    \n",
    "    Returns - The converted data and the class label for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpickle the data\n",
    "    data = _unpickle(filename)\n",
    "\n",
    "    # Retrieve the raw image pixel data\n",
    "    raw_images = data[b'data']\n",
    "\n",
    "    # Arrange the class labels into a numpy array\n",
    "    classes = np.array(data[b'labels'])\n",
    "\n",
    "    # Convert the image pixel size/orientation\n",
    "    images = _convert_images(raw_images)\n",
    "\n",
    "    return images, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btrJPpB44Bn_"
   },
   "outputs": [],
   "source": [
    "def load_class_names():\n",
    "\n",
    "    # Unpickle the file and access the class label names\n",
    "    raw_classes = _unpickle(filename=\"batches.meta\")[b'label_names']\n",
    "\n",
    "    # Convert from strings to a list\n",
    "    names = [x.decode('utf-8') for x in raw]\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IusKcNM04E8f"
   },
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    \"\"\"\n",
    "    Description - Builds numpy arrays containing the image data and the class \n",
    "                  labels from the 5 files in the Cifar 100 dataset.\n",
    "\n",
    "    Returns - The images and class labels for each training image data point \n",
    "              in the set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Allocate memory for the images\n",
    "    images = np.zeros(shape=[_n_images_train, n_pixels, n_pixels, n_channels], dtype=float)\n",
    "\n",
    "    # Allocate memory for the class labels\n",
    "    classes = np.zeros(shape=[_n_images_train], dtype=int)\n",
    "\n",
    "    # Index that is specific to the current batch\n",
    "    start = 0\n",
    "\n",
    "    # Loop through each of the 5 data files\n",
    "    for i in range(_n_files_train):\n",
    "\n",
    "        # Load the images and class labels from the 5 data files\n",
    "        images_batch, classes_batch = _load_data(filename=\"train\")\n",
    "\n",
    "        # This is the number of images contained in this batch\n",
    "        n_images_batch = len(images_batch)\n",
    "\n",
    "        # End index for the current batch.\n",
    "        end = start + n_images_batch\n",
    "\n",
    "        # Fill the empty array allocated above with the image data loaded above\n",
    "        images[begin:end, :] = images_batch\n",
    "\n",
    "        # Fill the empty array allocated above with the classes loaded above\n",
    "        classes[begin:end] = classes_batch\n",
    "\n",
    "        # For the next iteration, start at the end of the previously loaded data\n",
    "        begin = end\n",
    "\n",
    "    return images, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cW0LqhDD4Gax"
   },
   "outputs": [],
   "source": [
    "def load_validation_data():\n",
    "    \"\"\"\n",
    "    Description - Loads 5000 data points from the test batch file of the Cifar\n",
    "                  100 dataset to be used for validation.\n",
    "\n",
    "    Returns - The images and class labels for each validation image data point \n",
    "              in the set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the images and class labels from the test batch file\n",
    "    images, classes = _load_data(filename=\"test_batch\")\n",
    "\n",
    "    # Define the validation data to be all but the first 5000 data points\n",
    "    images = images[5000:, :, :, :]\n",
    "    classes = classes[5000:]\n",
    "\n",
    "    return images, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kdBU4NI4IbF"
   },
   "outputs": [],
   "source": [
    "def load_testing_data():\n",
    "    \"\"\"\n",
    "    Description - Loads 5000 data points from the test batch file of the Cifar\n",
    "                  100 dataset to be used for testing.\n",
    "\n",
    "    Returns - The images and class labels for each testing image data point \n",
    "              in the set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the unpickled and converted data\n",
    "    images, classes = _load_data(filename=\"test_batch\")\n",
    "\n",
    "    # Define the testing data to be the first 5000 data points\n",
    "    images = images[:5000, :, :, :]\n",
    "    classes = classes[:5000]\n",
    "\n",
    "    return images, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1575162693828,
     "user": {
      "displayName": "Anthony Lathrop",
      "photoUrl": "",
      "userId": "09824446360736258535"
     },
     "user_tz": 480
    },
    "id": "_Fv7yyYfAti4",
    "outputId": "c20d81a6-0c0b-4d01-de5f-9ef289fdffe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been saved locally and extracted.\n"
     ]
    }
   ],
   "source": [
    "# Get the Cifar 100 data\n",
    "download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 699,
     "status": "error",
     "timestamp": 1575162700427,
     "user": {
      "displayName": "Anthony Lathrop",
      "photoUrl": "",
      "userId": "09824446360736258535"
     },
     "user_tz": 480
    },
    "id": "4piXbth2A0Tn",
    "outputId": "6e9fea56-b499-40d3-9ccb-8c668ea311a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently loading data from: /tmp/data/cifar-100-python/train\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-af109fa8f161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-b968535c3fc9>\u001b[0m in \u001b[0;36mload_training_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Load the images and class labels from the 5 data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mimages_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# This is the number of images contained in this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-7ff6e5a7cfe3>\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Unpickle the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Retrieve the raw image pixel data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-08619b21aa45>\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Open the file located at file_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Load the data into a new variable using Pickle's load method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/data/cifar-100-python/train'"
     ]
    }
   ],
   "source": [
    "# Set model variables\n",
    "X_train, y_train = load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwUL6F6C4LGZ"
   },
   "outputs": [],
   "source": [
    "# Set model variables\n",
    "\n",
    "# Allocate memory for the images\n",
    "images = np.zeros(shape=[_n_images_train, n_pixels, n_pixels, n_channels], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duNIETc17QnR"
   },
   "outputs": [],
   "source": [
    "# Allocate memory for the class labels\n",
    "classes = np.zeros(shape=[_n_images_train], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vyd_vumd7T_y"
   },
   "outputs": [],
   "source": [
    "# Index that is specific to the current batch\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfN5DqtQ7Unq"
   },
   "outputs": [],
   "source": [
    "# Get the file path\n",
    "file_path = os.path.join(params.data_dir, \"cifar-100-python/\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1575162644099,
     "user": {
      "displayName": "Anthony Lathrop",
      "photoUrl": "",
      "userId": "09824446360736258535"
     },
     "user_tz": 480
    },
    "id": "f28nOiop8mpn",
    "outputId": "f6e8a86f-5f37-439b-9443-d8ee57fecf5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently loading data from: /tmp/data/cifar-100-python/train\n"
     ]
    }
   ],
   "source": [
    "# Print a status message\n",
    "print(\"Currently loading data from: \" + file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1025,
     "status": "error",
     "timestamp": 1575162648536,
     "user": {
      "displayName": "Anthony Lathrop",
      "photoUrl": "",
      "userId": "09824446360736258535"
     },
     "user_tz": 480
    },
    "id": "T81mfHQL85x_",
    "outputId": "4e91bae9-900a-4ff5-edca-1a4aa91886cc"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-71c443de51cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0;31m# Load the data into a new variable using Pickle's load method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/data/cifar-100-python/train'"
     ]
    }
   ],
   "source": [
    "# Open the file located at file_path\n",
    "with open(file_path, mode='rb') as file:\n",
    "\n",
    "     # Load the data into a new variable using Pickle's load method\n",
    "     data = pickle.load(file, encoding='bytes')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_EDA_Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
